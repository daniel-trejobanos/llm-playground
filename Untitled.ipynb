{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "caeb9664-55e9-463b-952d-88131e9b2689",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 8/8 [00:08<00:00,  1.12s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "model_name = \"epfl-llm/meditron-7b\"  \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "798d1e79-5951-42fd-8f65-9f08d9a984fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step one\n",
      "step two\n",
      "step 3\n",
      "Let's think step-by-step. What are standard treatments against ovarian cancer?\n",
      "- Surgery is the first line of treatment. It is used to remove as much of the tumor as possible. The surgeon removes the uterus, the fallopian tubes, and the ovary on the affected side. If the cancer has spread to the lymph nodes in the pelvis, they are also removed. This is called a total hysterectomy with bilateral salpingo-oophrectomy (THBSO). This surgery can be done laparoscopically or through a large abdominal incision (laparotomy). In some cases, it is not possible to completely remove the disease. In this case, a biopsy is performed to confirm the diagnosis and to determine the type of cancer.\n",
      "\n",
      "## - \n",
      "Chemotherapy is a treatment that uses drugs to kill cancer cells. Chemotherapeutic agents are given intravenously (through a vein) or orally (by mouth). The most commonly used chemotherapies are carboplatin, cisplatin, paclitaxel,\n"
     ]
    }
   ],
   "source": [
    "def generate_cot_response(prompt):\n",
    "  \"\"\"\n",
    "  Generates a Chain-of-Thought (CoT) response using the Meditron model.\n",
    "\n",
    "  Args:\n",
    "    prompt: The input question or task.\n",
    "\n",
    "  Returns:\n",
    "    The generated CoT response.\n",
    "  \"\"\"\n",
    "  print(\"step one\")\n",
    "  # 1. Prompt the model for a CoT explanation:\n",
    "  cot_prompt = f\"Let's think step-by-step. {prompt}\"\n",
    "  input_ids = tokenizer(cot_prompt, return_tensors=\"pt\").input_ids\n",
    "  print(\"step two\")\n",
    "  # 2. Generate the CoT response:\n",
    "  generated_ids = model.generate(\n",
    "      input_ids=input_ids,\n",
    "      max_length=256,  # Adjust as needed\n",
    "      num_beams=5,     # Adjust for beam search\n",
    "      no_repeat_ngram_size=2,\n",
    "      early_stopping=True\n",
    "  )\n",
    "  print(\"step 3\")\n",
    "  # 3. Decode the generated response:\n",
    "  cot_response = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "\n",
    "  return cot_response\n",
    "\n",
    "# Example usage:\n",
    "question = \"What are standard treatments against ovarian cancer?\"\n",
    "cot_response = generate_cot_response(question)\n",
    "print(cot_response) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e53eb56f-2c29-4ae2-a683-edfc504c9726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-s get about byby-step about\n",
      " is the first of the?\n",
      "\n",
      "tensor([[[1.5687e-09, 8.4315e-10, 1.8103e-07,  ..., 5.1785e-06,\n",
      "          1.8418e-06, 1.1702e-05],\n",
      "         [7.2899e-09, 3.9186e-09, 2.1544e-07,  ..., 3.1517e-08,\n",
      "          3.2411e-07, 3.5569e-08],\n",
      "         [2.5702e-09, 2.3227e-08, 9.9168e-10,  ..., 5.5458e-08,\n",
      "          5.3238e-08, 7.4496e-08],\n",
      "         ...,\n",
      "         [6.0340e-09, 3.9582e-09, 1.1753e-06,  ..., 2.7216e-07,\n",
      "          1.6272e-07, 1.3918e-07],\n",
      "         [2.3242e-09, 1.9306e-08, 1.9580e-08,  ..., 2.8468e-07,\n",
      "          7.0467e-07, 3.1991e-08],\n",
      "         [3.2856e-09, 1.4302e-08, 7.5604e-07,  ..., 8.9140e-07,\n",
      "          2.4390e-06, 4.6813e-07]]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "def get_softmax_values(prompt):\n",
    "  \"\"\"\n",
    "  Generates a Chain-of-Thought (CoT) response using the Meditron model \n",
    "  and returns the softmax values of the logits.\n",
    "\n",
    "  Args:\n",
    "    prompt: The input question or task.\n",
    "\n",
    "  Returns:\n",
    "    A tuple containing:\n",
    "      - The generated CoT response.\n",
    "      - A tensor containing the softmax values of the logits.\n",
    "  \"\"\"\n",
    "\n",
    "  # 1. Prepare input:\n",
    "  cot_prompt = f\"Let's think step-by-step. {prompt}\"\n",
    "  input_ids = tokenizer(cot_prompt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "  # 2. Generate the CoT response:\n",
    "  outputs = model(input_ids)\n",
    "  logits = outputs.logits \n",
    "\n",
    "  # 3. Apply softmax:\n",
    "  softmax_probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "  # 4. Decode the generated response:\n",
    "  generated_ids = torch.argmax(logits, dim=-1)\n",
    "  cot_response = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "\n",
    "  return cot_response, softmax_probs\n",
    "\n",
    "# Example usage:\n",
    "question = \"What is the capital of France?\"\n",
    "cot_response, softmax_probs = get_softmax_values(question)\n",
    "print(cot_response) \n",
    "print(softmax_probs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fbb29591-537d-4af3-8655-b7f8417c2310",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 18, 32017])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "softmax_probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ab36e7dd-aeee-45d5-9d6e-aa9688d9515d",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "int() argument must be a string, a bytes-like object or a real number, not 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 48\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# Example usage:\u001b[39;00m\n\u001b[1;32m     47\u001b[0m question \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat is the capital of France?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 48\u001b[0m token_prob_pairs \u001b[38;5;241m=\u001b[39m \u001b[43mget_tokens_with_probs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m token, prob \u001b[38;5;129;01min\u001b[39;00m token_prob_pairs:\n\u001b[1;32m     51\u001b[0m   \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mToken: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtoken\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Probability: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprob\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \n",
      "Cell \u001b[0;32mIn[11], line 37\u001b[0m, in \u001b[0;36mget_tokens_with_probs\u001b[0;34m(prompt, top_k)\u001b[0m\n\u001b[1;32m     34\u001b[0m top_k_probs, top_k_indices \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtopk(softmax_probs, k\u001b[38;5;241m=\u001b[39mtop_k, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# 5. Decode the top-k tokens:\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m top_k_tokens \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_ids_to_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtop_k_indices\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# 6. Combine tokens and probabilities:\u001b[39;00m\n\u001b[1;32m     40\u001b[0m token_prob_pairs \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py:404\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast.convert_ids_to_tokens\u001b[0;34m(self, ids, skip_special_tokens)\u001b[0m\n\u001b[1;32m    402\u001b[0m tokens \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    403\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index \u001b[38;5;129;01min\u001b[39;00m ids:\n\u001b[0;32m--> 404\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    405\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m skip_special_tokens \u001b[38;5;129;01mand\u001b[39;00m index \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_special_ids:\n\u001b[1;32m    406\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: int() argument must be a string, a bytes-like object or a real number, not 'list'"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "def get_tokens_with_probs(prompt, top_k=5):\n",
    "  \"\"\"\n",
    "  Generates a Chain-of-Thought (CoT) response using the Meditron model \n",
    "  and returns the top-k most probable tokens at each position.\n",
    "\n",
    "  Args:\n",
    "    prompt: The input question or task.\n",
    "    top_k: The number of top probable tokens to return.\n",
    "\n",
    "  Returns:\n",
    "    A list of tuples, where each tuple contains:\n",
    "      - The generated token.\n",
    "      - The probability of the generated token.\n",
    "  \"\"\"\n",
    "\n",
    "  # 1. Prepare input:\n",
    "  cot_prompt = f\"Let's think step-by-step. {prompt}\"\n",
    "  input_ids = tokenizer(cot_prompt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "  # 2. Generate the CoT response:\n",
    "  outputs = model(input_ids)\n",
    "  logits = outputs.logits \n",
    "\n",
    "  # 3. Apply softmax:\n",
    "  softmax_probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "  # 4. Get top-k probabilities and corresponding tokens:\n",
    "  top_k_probs, top_k_indices = torch.topk(softmax_probs, k=top_k, dim=-1)\n",
    "\n",
    "  # 5. Decode the top-k tokens:\n",
    "  top_k_tokens = tokenizer.convert_ids_to_tokens(top_k_indices[0].tolist())\n",
    "\n",
    "  # 6. Combine tokens and probabilities:\n",
    "  token_prob_pairs = []\n",
    "  for i in range(top_k):\n",
    "    token_prob_pairs.append((top_k_tokens[i], top_k_probs[0][i].item()))\n",
    "\n",
    "  return token_prob_pairs\n",
    "\n",
    "# Example usage:\n",
    "question = \"What is the capital of France?\"\n",
    "token_prob_pairs = get_tokens_with_probs(question)\n",
    "\n",
    "for token, prob in token_prob_pairs:\n",
    "  print(f\"Token: {token}, Probability: {prob:.4f}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cdf41ba7-5bac-4b8e-b2df-301d4b3557d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([(['▁', '▁O', '▁S', '▁C', '▁B'],\n",
       "   tensor([[0.3874, 0.0321, 0.0215, 0.0204, 0.0185]], grad_fn=<SelectBackward0>)),\n",
       "  (['-', 'ter', \"'\", 'roz', 'ters'],\n",
       "   tensor([[0.3440, 0.3117, 0.0415, 0.0392, 0.0271]], grad_fn=<SelectBackward0>)),\n",
       "  (['s', '▁s', 'S', 'er', '▁'],\n",
       "   tensor([[9.9246e-01, 1.9348e-03, 7.2038e-04, 1.8667e-04, 1.7605e-04]],\n",
       "          grad_fn=<SelectBackward0>)),\n",
       "  (['▁get', '▁talk', '▁Get', '▁T', '▁face'],\n",
       "   tensor([[0.0815, 0.0699, 0.0475, 0.0443, 0.0305]], grad_fn=<SelectBackward0>)),\n",
       "  (['▁about', '▁of', '▁for', '▁this', '▁outside'],\n",
       "   tensor([[0.5842, 0.0869, 0.0216, 0.0160, 0.0150]], grad_fn=<SelectBackward0>))],\n",
       " '-s get about byby-step about\\n is the first of the?\\n')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def get_tokens_with_probs(prompt, top_k=5):\n",
    "  \"\"\"\n",
    "  Generates a Chain-of-Thought (CoT) response using the Meditron model \n",
    "  and returns the top-k most probable tokens at each position.\n",
    "\n",
    "  Args:\n",
    "    prompt: The input question or task.\n",
    "    top_k: The number of top probable tokens to return.\n",
    "\n",
    "  Returns:\n",
    "    A list of tuples, where each tuple contains:\n",
    "      - The generated token.\n",
    "      - The probability of the generated token.\n",
    "  \"\"\"\n",
    "\n",
    "  # 1. Prepare input:\n",
    "  cot_prompt = f\"Let's think step-by-step. {prompt}\"\n",
    "  input_ids = tokenizer(cot_prompt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "  # 2. Generate the CoT response:\n",
    "  outputs = model(input_ids)\n",
    "  logits = outputs.logits \n",
    "\n",
    "  # 3. Apply softmax:\n",
    "  softmax_probs = F.softmax(logits, dim=-1)\n",
    "  generated_ids = torch.argmax(logits, dim=-1)\n",
    "  cot_response = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "\n",
    "  cot_response = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "  # 4. Get top-k probabilities and corresponding tokens:\n",
    "  top_k_probs, top_k_indices = torch.topk(softmax_probs, k=top_k, dim=-1)\n",
    "  top_k_tokens = []\n",
    "  for i in range(top_k_indices.size(0)):  # Iterate over batch dimension\n",
    "      decoded_tokens = []\n",
    "      for token in top_k_indices[i].tolist():\n",
    "          decoded_tokens.append(tokenizer.convert_ids_to_tokens(token))\n",
    "      top_k_tokens.append(decoded_tokens) \n",
    "\n",
    "  # 6. Combine tokens and probabilities:\n",
    "  token_prob_pairs = []\n",
    "  for i in range(top_k):\n",
    "    token_prob_pairs.append((top_k_tokens[0][i], top_k_probs[:,i]))  # Access the first sequence in the batch\n",
    " \n",
    "  return token_prob_pairs, cot_response\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "question = \"What is the capital of France?\"\n",
    "get_tokens_with_probs(question)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "16150d9f-6d4b-435c-bace-e555a2d79a11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'▁best'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(1900)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce09229e-1124-40a3-996b-dd9d58b44dfa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
